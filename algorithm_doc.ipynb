{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation in probibilistic model\n",
    "\n",
    "Assume data generate via a probabilistic model :\n",
    "\\begin{align}\n",
    "d \\sim P(d|\\theta)\n",
    "\\end{align}\n",
    "\n",
    "$P(d|\\theta)$ : Probability distribution underlying the data\n",
    "* $\\theta$ : fixed, but unknown distribution parameter\n",
    "\n",
    "Given $n$ independent and identically distributed samples of data $D$ :\n",
    "\\begin{align}\n",
    "D = \\{d_1, d_2, d_3, ..., d_n\\}\n",
    "\\end{align}\n",
    "\n",
    "**To estimate parameter $\\theta$ that best describes the data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The maximum-a-posteriori estimation\n",
    "\n",
    "**Maximum-a-Posteriori** (**MAP**) is to choose $\\theta$ which maximizes the posterior probability of $\\theta$.\n",
    "\n",
    "**Posterior probability of $\\theta$** is given by the Bayes Rule :\n",
    "\\begin{align}\n",
    "P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}\n",
    "\\end{align}\n",
    "\n",
    "where, \n",
    "> $p(D|\\theta)$ : **likelihood function**\n",
    ">\n",
    "> $p(\\theta)$ : **prior probapility of $\\theta$**, without having seen any data\n",
    ">\n",
    "> $p(D)$ : **probapility of data**, independent of $\\theta$\n",
    "\n",
    "**Posterior probability of $\\theta$** could be driven as :\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta}_{MAP} = \\mathop{\\arg\\max}_{\\theta}P(\\theta|D) &= \\mathop{\\arg\\max}_{\\theta}\\frac{P(D|\\theta)P(\\theta)}{P(D)}\\\\\n",
    "&= \\mathop{\\arg\\max}_{\\theta}P(D|\\theta)P(\\theta)\\\\\n",
    "&= \\mathop{\\arg\\max}_{\\theta}log(P(D|\\theta)P(\\theta))\\\\\n",
    "&= \\mathop{\\arg\\max}_{\\theta}[logP(D|\\theta) + logP(\\theta)]\\\\\n",
    "\\hat{\\theta}_{MAP} &= \\mathop{\\arg\\max}_{\\theta}[\\mathop{\\sum}^{n}_{i=1}logP(d_i|\\theta) + logP(\\theta)]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "The logistic regression model is :\n",
    "\n",
    "\\begin{align}\n",
    "p(y=\\pm1|\\mathbf{x}, \\mathbf{w}) & = \\frac{1}{1+e^{-y\\mathbf{w}^T\\mathbf{x}}} \n",
    "\\end{align}\n",
    "\n",
    "A common prior to use with MAP is :\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{w}) & \\sim \\mathcal{N}(0, \\lambda^{-1}\\mathbf{I}) \n",
    "\\end{align}\n",
    "\n",
    "where $\\lambda$ is regularization strength\n",
    "\n",
    "Given a data set $(\\mathbf{X}, \\mathbf{y}) = [({\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_{1}, y_1), ({\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_2, y_2), ..., ({\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_n, y_n)]$, we want to find the parameter vector $\\mathop{\\mathbf{w}}^{\\rightharpoonup}$ which maximizes :\n",
    "\n",
    "\\begin{align}\n",
    "l(\\mathop{\\mathbf{w}}^{\\rightharpoonup}) = -\\mathop{\\sum}^{n}_{i=1}log(1+e^{-y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}) - \\frac{\\lambda}{2}{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Driven :\n",
    ">\n",
    ">data set $(\\mathbf{X}, \\mathbf{y}) = [({\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_{1}, y_1), ({\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_2, y_2), ..., ({\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_n, y_n)]$ could be seen as $D = \\{d_1, d_2, d_3, ..., d_n\\}$\n",
    ">\n",
    ">the parameter vector $\\mathop{\\mathbf{w}}^{\\rightharpoonup}$ was $\\theta$\n",
    ">\n",
    ">so the maximum posterior probability of the parameter vector $\\mathop{\\mathbf{w}}^{\\rightharpoonup}$ could be written as : \n",
    ">\n",
    ">\\begin{align}\n",
    "{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}_{MAP} &= \\mathop{\\arg\\max}_{\\theta}[\\mathop{\\sum}^{n}_{i=1}logP(y_i, {\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_{i}|\\mathop{\\mathbf{w}}^{\\rightharpoonup}) + logP(\\mathop{\\mathbf{w}}^{\\rightharpoonup})]\n",
    "\\end{align}\n",
    ">\n",
    "><p style=\"border:2px solid rgb(235, 200, 10); width:70%;margin:auto\">\\begin{align}\n",
    "\\text{Recalled : }&\\text{ Normal probability density function :} &&p(x) = \\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\\\\n",
    "&\\text{Prior probability of $\\mathop{\\mathbf{w}}^{\\rightharpoonup}$ :} &&p(w)\\sim\\mathcal{N}(0,\\lambda^{-1}) = \\frac{1}{\\sqrt{\\frac{2\\pi}{\\lambda}}}e^{-\\frac{\\lambda(w-0)^2}{2}}\n",
    "\\end{align}</p>\n",
    ">\n",
    ">\\begin{align}\n",
    "{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}_{MAP} &= \\mathop{\\arg\\max}_{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}\\mathop{\\sum}^{n}_{i=1}log(\\frac{1}{1+e^{-y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}}) + log(\\frac{1}{\\sqrt{\\frac{2\\pi}{\\lambda}}}e^{-\\frac{\\lambda {\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}}{2}})\\\\\n",
    "&= \\mathop{\\arg\\max}_{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}\\mathop{\\sum}^{n}_{i=1}log({1+e^{-y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}})^{-1} + [log(\\frac{1}{\\sqrt{\\frac{2\\pi}{\\lambda}}}) + log(e^{-\\frac{\\lambda {\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}}{2}})]\\\\\n",
    "&= \\mathop{\\arg\\max}_{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}-\\mathop{\\sum}^{n}_{i=1}log({1+e^{-y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}}) - \\frac{\\lambda}{2}{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's method\n",
    "Newton's method is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function.\n",
    "\n",
    "\\begin{align}\n",
    "x:f(x)=0\n",
    "\\end{align}\n",
    "\n",
    "The process is repeated as :\n",
    "\n",
    "\\begin{align}\n",
    "x_{new} = x_{old} - \\frac{f(x_{old})}{f'(x_{old})}\n",
    "\\end{align}\n",
    "\n",
    "```java\n",
    "if abs(w_new - w_old) < tolerance\n",
    "    break\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">known a curve :\n",
    ">\n",
    ">\\begin{align}\n",
    "y = f(x)\n",
    "\\end{align}\n",
    ">\n",
    ">a point at $x_n$ had a $y_n$ :\n",
    ">\n",
    ">\\begin{align}\n",
    "y_n = f(x_n)\n",
    "\\end{align}\n",
    ">\n",
    ">and another point $x$ approximate to $x_n$ had a $y$ :\n",
    ">\n",
    ">\\begin{align}\n",
    "y = f(x_n) + \\frac{f(x) - f(x_n)}{(x - x_n)}(x - x_n)\n",
    "\\end{align}\n",
    ">\n",
    ">the $\\frac{f(x) - f(x_n)}{(x - x_n)}$ is the tangent line at $x_n$\n",
    ">\n",
    ">\\begin{align}\n",
    "y = f(x_n) + f'(x_n)(x - x_n)\n",
    "\\end{align}\n",
    ">\n",
    ">where $f'$ denotes the derivative of the function $f$\n",
    ">\n",
    ">the root, the value of $x$ such that $y = 0$, is then used as the next approximation to the root, $x_{n+1}$ : \n",
    ">\n",
    ">\\begin{align}\n",
    "0 = f(x_n) + f'(x_n)(x_{n+1} - x_n)\n",
    "\\end{align}\n",
    ">\n",
    ">solving for $x_{n+1}$ gives\n",
    ">\\begin{align}\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To find the maxima of posterior probability function\n",
    "\n",
    "Given $f(x)$, we **differentiate once** to find $f'(x)$. \n",
    "\n",
    "Set $f'(x) = 0$ and solve for $x$. Using our above observation, the x values we find are the $x$-coordinates of our maxima and minima.\n",
    "\n",
    "Substitute these $x$-values back into $f(x)$. This gives the corresponding $y$-coordinates of our maxima or minima.\n",
    "\n",
    "Since, to find the maxima of posterior probability function is equivalent **to find the root of the gradient of posterior probability function**.\n",
    "\n",
    "Then use of the gradient of the objective and Hessian matrix to converge in Newton's iterative method to find the maxima of posterior probability function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior probability function : \n",
    "\n",
    "\\begin{align}\n",
    "l(\\mathop{\\mathbf{w}}^{\\rightharpoonup}) = -\\mathop{\\sum}^{n}_{i=1}log(1+e^{-y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}) - \\frac{\\lambda}{2}{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}\n",
    "\\end{align}\n",
    "\n",
    "The gradient of the objective, **first-order partial derivatives** of the posterior function :\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{g} = \\triangledown_{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}l(\\mathop{\\mathbf{w}}^{\\rightharpoonup}) = \\mathop{\\sum}^{n}_{i=1}(1-\\frac{1}{1+e^{-y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}})y_i{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i - \\lambda{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}\n",
    "\\end{align}\n",
    "\n",
    "The Hessian matrix, a square matrix of **second-order partial derivatives** of the posterior function :\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{H} = \\frac{d^2l(\\mathop{\\mathbf{w}}^{\\rightharpoonup})}{d\\mathop{\\mathbf{w}}^{\\rightharpoonup}d{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T} = -\\mathop{\\sum}^{n}_{i=1}(\\frac{1}{1+e^{-{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}})(1-\\frac{1}{1+e^{-{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}}){\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}^T_i - \\lambda\\mathbf{I}\n",
    "\\end{align}\n",
    "\n",
    "which in matrix form can be written :\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{H} = -\\mathbf{XAX}^{T} - \\lambda\\mathbf{I}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{A}$ is a diagonal matrix :\n",
    "\\begin{align}\n",
    "a_{ii} = \\frac{1}{1+e^{{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}}(1-\\frac{1}{1+e^{{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's step\n",
    "\n",
    "\\begin{align}\n",
    "{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}_{new} &= {\\mathop{\\mathbf{w}}^{\\rightharpoonup}}_{old} - \\mathbf{H}^{-1}\\mathbf{g}\\\\\n",
    "&= {\\mathop{\\mathbf{w}}^{\\rightharpoonup}}_{old} + ({\\mathbf{XAX}^T+\\lambda\\mathbf{I}})^{-1}(\\mathop{\\sum}^{n}_{i=1}(1-\\frac{1}{1+e^{-y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}})y_i{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i - \\lambda{\\mathop{\\mathbf{w}}^{\\rightharpoonup}})\\\\\n",
    "&=({\\mathbf{XAX}^T+\\lambda\\mathbf{I}})^{-1}\\mathbf{XA}(\\mathbf{X}^T{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}_{old}+\\frac{1-\\frac{y_i}{1+e^{y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}}}{a_{ii}})\n",
    "\\end{align}\n",
    "\n",
    "If we define :\n",
    "\n",
    "\\begin{align}\n",
    "z_i =\\mathbf{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}^T_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}_{old}+\\frac{1-\\frac{y_i}{1+e^{y_i{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}^T{\\mathop{\\mathbf{x}}^{\\rightharpoonup}}_i}}}{a_{ii}}\n",
    "\\end{align}\n",
    "\n",
    "The Newton's process is :\n",
    "\\begin{align}\n",
    "{\\mathop{\\mathbf{w}}^{\\rightharpoonup}}_{new}=({\\mathbf{XAX}^T+\\lambda\\mathbf{I}})^{-1}\\mathbf{XA}\\mathop{\\mathbf{z}}^{\\rightharpoonup}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* Minka, T. P. (2003). A comparison of numerical optimizers for logistic regression. Unpublished draft.\n",
    "* Grus, J. (2015). Data science from scratch: first principles with python (pp.270-279). \" O'Reilly Media, Inc.\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
